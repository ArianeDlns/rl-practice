{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0c70d56",
   "metadata": {},
   "source": [
    "# TD Deep Q-Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898a86dc",
   "metadata": {},
   "source": [
    "# Tutorial - Deep Q-Learning \n",
    "\n",
    "Deep Q-Learning uses a neural network to approximate $Q$ functions. Hence, we usually refer to this algorithm as DQN (for *deep Q network*).\n",
    "\n",
    "The parameters of the neural network are denoted by $\\theta$. \n",
    "*   As input, the network takes a state $s$,\n",
    "*   As output, the network returns $Q_\\theta [a | s] = Q_\\theta (s,a) = Q(s, a, \\theta)$, the value of each action $a$ in state $s$, according to the parameters $\\theta$.\n",
    "\n",
    "\n",
    "The goal of Deep Q-Learning is to learn the parameters $\\theta$ so that $Q(s, a, \\theta)$ approximates well the optimal $Q$-function $Q^*(s, a) \\simeq Q_{\\theta^*} (s,a)$. \n",
    "\n",
    "In addition to the network with parameters $\\theta$, the algorithm keeps another network with the same architecture and parameters $\\theta^-$, called **target network**.\n",
    "\n",
    "The algorithm works as follows:\n",
    "\n",
    "1.   At each time $t$, the agent is in state $s_t$ and has observed the transitions $(s_i, a_i, r_i, s_i')_{i=1}^{t-1}$, which are stored in a **replay buffer**.\n",
    "\n",
    "2.  Choose action $a_t = \\arg\\max_a Q_\\theta(s_t, a)$ with probability $1-\\varepsilon_t$, and $a_t$=random action with probability $\\varepsilon_t$. \n",
    "\n",
    "3. Take action $a_t$, observe reward $r_t$ and next state $s_t'$.\n",
    "\n",
    "4. Add transition $(s_t, a_t, r_t, s_t')$ to the **replay buffer**.\n",
    "\n",
    "4.  Sample a minibatch $\\mathcal{B}$ containing $B$ transitions from the replay buffer. Using this minibatch, we define the loss:\n",
    "\n",
    "$$\n",
    "L(\\theta) = \\sum_{(s_i, a_i, r_i, s_i') \\in \\mathcal{B}}\n",
    "\\left[\n",
    "Q(s_i, a_i, \\theta) -  y_i\n",
    "\\right]^2\n",
    "$$\n",
    "where the $y_i$ are the **targets** computed with the **target network** $\\theta^-$:\n",
    "\n",
    "$$\n",
    "y_i = r_i + \\gamma \\max_{a'} Q(s_i', a', \\theta^-).\n",
    "$$\n",
    "\n",
    "5. Update the parameters $\\theta$ to minimize the loss, e.g., with gradient descent (**keeping $\\theta^-$ fixed**): \n",
    "$$\n",
    "\\theta \\gets \\theta - \\eta \\nabla_\\theta L(\\theta)\n",
    "$$\n",
    "where $\\eta$ is the optimization learning rate. \n",
    "\n",
    "6. Every $N$ transitions ($t\\mod N$ = 0), update target parameters: $\\theta^- \\gets \\theta$.\n",
    "\n",
    "7. $t \\gets t+1$. Stop if $t = T$, otherwise go to step 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3192463f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from copy import deepcopy\n",
    "import gym\n",
    "from gym.wrappers import Monitor\n",
    "\n",
    "# from pyvirtualdisplay import Display\n",
    "from IPython import display as ipythondisplay\n",
    "from IPython.display import clear_output\n",
    "from pathlib import Path\n",
    "import base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8ea50e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python --version = 3.8.10 (v3.8.10:3d8993a744, May  3 2021, 09:09:08) \n",
      "[Clang 12.0.5 (clang-1205.0.22.9)]\n",
      "torch.__version__ = 1.10.1\n",
      "np.__version__ = 1.21.4\n",
      "gym.__version__ = 0.21.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"python --version = {sys.version}\")\n",
    "print(f\"torch.__version__ = {torch.__version__}\")\n",
    "print(f\"np.__version__ = {np.__version__}\")\n",
    "print(f\"gym.__version__ = {gym.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4117afb1",
   "metadata": {},
   "source": [
    "## Torch 101\n",
    "\n",
    ">\"The torch package contains data structures for multi-dimensional tensors and defines mathematical operations over these tensors. Additionally, it provides many utilities for efficient serializing of Tensors and arbitrary types, and other useful utilities. \n",
    "[...] provides classes and functions implementing automatic differentiation of arbitrary scalar valued functions.\" \n",
    "[PyTorch](https://pytorch.org/docs/stable/index.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f94aaf",
   "metadata": {},
   "source": [
    "### Variable types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2ea2eaa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zero_torch is of type <class 'torch.Tensor'>\n",
      "\n",
      "Float:\n",
      " tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]])\n",
      "Int:\n",
      " tensor([[0, 0],\n",
      "        [0, 0],\n",
      "        [0, 0]])\n",
      "Bool:\n",
      " tensor([[False, False],\n",
      "        [False, False],\n",
      "        [False, False]])\n",
      "\n",
      "View new shape... tensor([[0., 0., 0., 0., 0., 0.]])\n",
      "\n",
      "Algebraic operations are overloaded:\n",
      " tensor([[ 0.3009,  1.4382],\n",
      "        [-0.9523,  0.0252],\n",
      "        [-0.9045,  0.8632]]) \n",
      "+\n",
      " tensor([[ 1.2569,  0.5458],\n",
      "        [ 0.3136,  0.6893],\n",
      "        [-0.9142,  1.6274]]) \n",
      "=\n",
      " tensor([[ 1.5578,  1.9840],\n",
      "        [-0.6387,  0.7145],\n",
      "        [-1.8186,  2.4906]])\n"
     ]
    }
   ],
   "source": [
    "# Very similar syntax to numpy.\n",
    "zero_torch = torch.zeros((3, 2))\n",
    "\n",
    "print('zero_torch is of type {:s}'.format(str(type(zero_torch))))\n",
    "\n",
    "# Torch -> Numpy: simply call the numpy() method.\n",
    "zero_np = np.zeros((3, 2))\n",
    "assert (zero_torch.numpy() == zero_np).all()\n",
    "\n",
    "# Numpy -> Torch: simply call the corresponding function on the np.array.\n",
    "zero_torch_float = torch.FloatTensor(zero_np)\n",
    "print('\\nFloat:\\n', zero_torch_float)\n",
    "zero_torch_int = torch.LongTensor(zero_np)\n",
    "print('Int:\\n', zero_torch_int)\n",
    "zero_torch_bool = torch.BoolTensor(zero_np)\n",
    "print('Bool:\\n', zero_torch_bool)\n",
    "\n",
    "# Reshape\n",
    "print('\\nView new shape...', zero_torch.view(1, 6))\n",
    "# Note that print(zero_torch.reshape(1, 6)) would work too.\n",
    "# The difference is in how memory is handled (view imposes contiguity).\n",
    "\n",
    "# Algebra\n",
    "a = torch.randn((3, 2))\n",
    "b = torch.randn((3, 2))\n",
    "print('\\nAlgebraic operations are overloaded:\\n', a, '\\n+\\n', b, '\\n=\\n', a+b )\n",
    "\n",
    "# More generally, torch shares the syntax of many attributes and functions with Numpy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7f1191",
   "metadata": {},
   "source": [
    "### Gradient management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "29a2a715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]])\n",
      "Initial guess: tensor([-0.8437])\n",
      "Final estimate: tensor([1.9852])\n",
      "The final estimate should be close to tensor([2.])\n"
     ]
    }
   ],
   "source": [
    "# torch.Tensor is a similar yet more complicated data structure than np.array.\n",
    "# It is basically a static array of number but may also contain an overlay to \n",
    "# handle automatic differentiation (i.e keeping track of the gradient and which \n",
    "# tensors depend on which).\n",
    "# To access the static array embedded in a tensor, simply call the detach() method\n",
    "print(zero_torch.detach())\n",
    "\n",
    "# When inside a function performing automatic differentiation (basically when training \n",
    "# a neural network), never use detach() otherwise meta information regarding gradients\n",
    "# will be lost, effectively freezing the variable and preventing backprop for it. \n",
    "# However when returning the result of training, do use detach() to save memory \n",
    "# (the naked tensor data uses much less memory than the full-blown tensor with gradient\n",
    "# management, and is much less prone to mistake such as bad copy and memory leak).\n",
    "\n",
    "# We will solve theta * x = y in theta for x=1 and y=2\n",
    "x = torch.ones(1)\n",
    "y = 2 * torch.ones(1)\n",
    "\n",
    "# Actually by default torch does not add the gradient management overlay\n",
    "# when declaring tensors like this. To force it, add requires_grad=True.\n",
    "theta = torch.randn(1, requires_grad=True)\n",
    "\n",
    "# Optimisation routine\n",
    "# (Adam is a sophisticated variant of SGD, with adaptive step).\n",
    "optimizer = optim.Adam(params=[theta], lr=0.1)\n",
    "\n",
    "# Loss function\n",
    "print('Initial guess:', theta.detach())\n",
    "\n",
    "for _ in range(100):\n",
    "    # By default, torch accumulates gradients in memory.\n",
    "    # To obtain the desired gradient descent beahviour,\n",
    "    # just clean the cached gradients using the following line:\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Quadratic loss (* and ** are overloaded so that torch\n",
    "    # knows how to differentiate them)\n",
    "    loss = (y - theta * x) ** 2\n",
    "    \n",
    "    # Apply the chain rule to automatically compute gradients\n",
    "    # for all relevant tensors.\n",
    "    loss.backward()\n",
    "    \n",
    "    # Run one step of optimisation routine.\n",
    "    optimizer.step()\n",
    "    \n",
    "print('Final estimate:', theta.detach())\n",
    "print('The final estimate should be close to', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f014ed38",
   "metadata": {},
   "source": [
    "## Setting the environment\n",
    "\n",
    "### 1 - Define the GLOBAL parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "03a05cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "# Discount factor\n",
    "GAMMA = 0.99\n",
    "\n",
    "# Batch size\n",
    "BATCH_SIZE = 256\n",
    "# Capacity of the replay buffer\n",
    "BUFFER_CAPACITY = 16384 # 10000\n",
    "# Update target net every ... episodes\n",
    "UPDATE_TARGET_EVERY = 32 # 20\n",
    "\n",
    "# Initial value of epsilon\n",
    "EPSILON_START = 1.0\n",
    "# Parameter to decrease epsilon\n",
    "DECREASE_EPSILON = 100\n",
    "# Minimum value of epislon\n",
    "EPSILON_MIN = 0.05\n",
    "\n",
    "# Number of training episodes\n",
    "N_EPISODES = 1000\n",
    "\n",
    "# Learning rate\n",
    "LEARNING_RATE = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c38d6dd",
   "metadata": {},
   "source": [
    "### 2 - Replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "dbd61940",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, state, action, reward, next_state):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = (state, action, reward, next_state)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.choices(self.memory, k=batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "# create instance of replay buffer\n",
    "replay_buffer = ReplayBuffer(BUFFER_CAPACITY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a64a096",
   "metadata": {},
   "source": [
    "### 3 - Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0decdfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    Basic neural net.\n",
    "    \"\"\"\n",
    "    def __init__(self, obs_size, hidden_size, n_actions):\n",
    "        super(Net, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66eaf259",
   "metadata": {},
   "source": [
    "### 3.5 - Loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c4858d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create network and target network\n",
    "hidden_size = 128\n",
    "obs_size = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "q_net = Net(obs_size, hidden_size, n_actions)\n",
    "target_net = Net(obs_size, hidden_size, n_actions)\n",
    "\n",
    "# objective and optimizer\n",
    "objective = nn.MSELoss()\n",
    "optimizer = optim.Adam(params=q_net.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7975fdd2",
   "metadata": {},
   "source": [
    "### Question 0 (to do at home, not during the live session)\n",
    "\n",
    "With your own word, explain the intuition behind DQN. Recall the main parts of the aformentionned algorithm.\n",
    "\n",
    "A DQN, or Deep Q-Network, approximates a state-value function in a Q-Learning framework with a neural network. In the Atari Games case, they take in several frames of the game as an input and output state values for each action as an output.\n",
    "\n",
    "It is usually used in conjunction with Experience Replay, for storing the episode steps in memory for off-policy learning, where samples are drawn from the replay memory at random. Additionally, the Q-Network is usually optimized towards a frozen target network that is periodically updated with the latest weights every\n",
    "steps (where is a hyperparameter). The latter makes training more stable by preventing short-term oscillations from a moving target. The former tackles autocorrelation that would occur from on-line learning, and having a replay memory makes the problem more like a supervised learning problem.\n",
    "\n",
    "\n",
    "On se place dans une suite d'états s avec des actions a  \n",
    "L'objectif est à partir d'un état s exécuter l'action a qui amène à l'état s' qui à valeur maximale  \n",
    "Q prédit la valeur du couple (s,a) c'est à dire de l'état s'' qui pourrait être obtenu en faisant l'action a en s  \n",
    "A partir de Q on détermine la meilleure action à faire  \n",
    "Une fois l'état s' obtenu on ajuste Q suivant la formule du cours Q(s,a) = (1-aplha).Q(s,a) + alpha. (r+gamma.max_a'Q(s',a'))  \n",
    "On modélise Q avec un réseau de neurones pour palier au problème du grand nombre d'états (continu)  \n",
    "\n",
    "\n",
    "Réference: https://paperswithcode.com/method/dqn  \n",
    "https://github.com/gle-bellier/dueling-network-architectures/blob/8519a0932d6e55d0f5cf40dd841ad11dbc7edad5/notebooks/Deep_Q_Learning.ipynb\n",
    "https://github.com/amathsow/machine-learning/blob/master/DQN.ipynb  \n",
    "https://github.com/Volviane/Reinforcement-learning/blob/master/Practical_Session_DQN.ipynb  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b72807",
   "metadata": {},
   "source": [
    "## Implementing the DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "652a3c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_q(states):\n",
    "    \"\"\"\n",
    "    Compute Q function for a list of states\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        states_v = torch.FloatTensor(np.array([states]))\n",
    "        output = q_net.forward(states_v).detach().numpy()  # shape (1, len(states), n_actions)\n",
    "    return output[0, :, :]  # shape (len(states), n_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77eb2b2c",
   "metadata": {},
   "source": [
    "#### Question 1\n",
    "\n",
    "Implement the `choose_action` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1672f298",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state, epsilon):\n",
    "    \"\"\"\n",
    "    Return action according to an epsilon-greedy exploration policy\n",
    "    \"\"\"  \n",
    "    actions = get_q([state])[0]    \n",
    "    n_action = len(actions)    \n",
    "    exploration = np.random.random()    \n",
    "    if exploration < epsilon :        \n",
    "        return np.random.randint(0,n_action)    \n",
    "    else :        \n",
    "        return np.argmax(actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3302b69f",
   "metadata": {},
   "source": [
    "#### Question 2\n",
    "\n",
    "Implement the `eval_dqn` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9e15cae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_dqn(n_sim=5):\n",
    "    \"\"\"\n",
    "    Monte Carlo evaluation of DQN agent.\n",
    "\n",
    "    Repeat n_sim times:\n",
    "        * Run the DQN policy until the environment reaches a terminal state (= one episode)\n",
    "        * Compute the sum of rewards in this episode\n",
    "        * Store the sum of rewards in the episode_rewards array.\n",
    "    \"\"\"\n",
    "    env_copy = deepcopy(env)\n",
    "    episode_rewards = np.zeros(n_sim)\n",
    "    \n",
    "    for i in range(n_sim):\n",
    "        state = env_copy.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = choose_action(state, 0.0)\n",
    "            state, reward, done, _ = env_copy.step(action)\n",
    "            episode_rewards[i] += reward\n",
    "    return episode_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961f2b0a",
   "metadata": {},
   "source": [
    "#### Question 3\n",
    "\n",
    "Implement the `update` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2d1e6bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(state, action, reward, next_state, done):\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    # add data to replay buffer\n",
    "    if done:\n",
    "        next_state = None\n",
    "    replay_buffer.push(state, action, reward, next_state)\n",
    "\n",
    "    if len(replay_buffer) < BATCH_SIZE:\n",
    "        return np.inf\n",
    "\n",
    "    # get batch\n",
    "    transitions = replay_buffer.sample(BATCH_SIZE)\n",
    "\n",
    "   # 1st thing: compute Q(s_i, a_i, theta) for all (s_i, a_i)\n",
    "   # in the batch\n",
    "\n",
    "   # Build tensor with s_i and tensor with a_i\n",
    "    batch_states = torch.FloatTensor( \n",
    "                    [transitions[ii][0] for ii in range(BATCH_SIZE) ]\n",
    "                    )\n",
    "    batch_actions = torch.LongTensor(  # type is important (Long) \n",
    "                    [ transitions[ii][1] for ii in range(BATCH_SIZE) ]\n",
    "                    )\n",
    "    batch_rewards = torch.FloatTensor( \n",
    "                    [ transitions[ii][2] for ii in range(BATCH_SIZE) ]\n",
    "                    )\n",
    "    \n",
    "    non_final_mask = torch.tensor([(transitions[ii][3] is not None) \n",
    "                                   for ii in range(BATCH_SIZE)], dtype=torch.bool)\n",
    "    non_final_next_states = torch.FloatTensor(\n",
    "            [transitions[ii][3]  for ii in range(BATCH_SIZE) \n",
    "            if transitions[ii][3] is not None])\n",
    "        \n",
    "    next_state_values = torch.zeros(BATCH_SIZE)\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    state_action_values = q_net(batch_states).gather(1, batch_actions.view(-1, 1))\n",
    "    \n",
    "    # Compute loss - TO BE IMPLEMENTED!\n",
    "    values  = state_action_values\n",
    "    targets = batch_rewards + GAMMA*next_state_values\n",
    "    loss = objective(values, targets.unsqueeze(1))\n",
    "\n",
    "    # Compute loss - TO BE IMPLEMENTED!\n",
    "    # states = torch.FloatTensor([trans[0] for trans in transitions])\n",
    "    # rewards = torch.FloatTensor([trans[2] for trans in transitions])\n",
    "    # actions = torch.LongTensor([trans[1] for trans in transitions])\n",
    "    #\n",
    "    # next_states = torch.FloatTensor([transitions[i][3] for i in range(BATCH_SIZE) if transitions[i][3] is not None]) \n",
    "\n",
    "    # mask = [transitions[i][3] is not None for i in range(BATCH_SIZE)]\n",
    "\n",
    "    # states_q = q_net.forward(states)\n",
    "\n",
    "    # next_states_q = torch.zeros(BATCH_SIZE)\n",
    "    # next_states_q[mask] = target_net.forward(next_states).max(dim=1)[0].data \n",
    "    # next_states_q = next_states_q.reshape(-1, 1)\n",
    "\n",
    "    # targets = rewards + GAMMA*next_states_q # to be computed using batch\n",
    "    \n",
    "    # values = torch.gather(states_q, dim = 1, index = actions.reshape(-1,1)) \n",
    "    \n",
    "    # loss = objective(values, targets) \n",
    "    \n",
    "    # Optimize the model - UNCOMMENT!\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0152c68",
   "metadata": {},
   "source": [
    "#### Question 4\n",
    "Train a DQN on the `env` environment.\n",
    "*Hint* The mean reward after training should be close to 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5a59e924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode = 5 , reward =  30.4 , loss =  inf\n",
      "episode = 10 , reward =  28.0 , loss =  inf\n",
      "episode = 15 , reward =  12.0 , loss =  0.0010938346\n",
      "episode = 20 , reward =  24.4 , loss =  0.0004410166\n",
      "episode = 25 , reward =  19.0 , loss =  0.00052026217\n",
      "episode = 30 , reward =  19.8 , loss =  0.0004373184\n",
      "episode = 35 , reward =  9.4 , loss =  0.047970887\n",
      "episode = 40 , reward =  11.0 , loss =  0.03529239\n",
      "episode = 45 , reward =  9.2 , loss =  0.017626038\n",
      "episode = 50 , reward =  9.6 , loss =  0.01493181\n",
      "episode = 55 , reward =  9.6 , loss =  0.0053276615\n",
      "episode = 60 , reward =  10.2 , loss =  0.010620292\n",
      "episode = 65 , reward =  18.0 , loss =  0.010024826\n",
      "episode = 70 , reward =  11.0 , loss =  0.02702087\n",
      "episode = 75 , reward =  24.4 , loss =  0.026813712\n",
      "episode = 80 , reward =  9.4 , loss =  0.017693534\n",
      "episode = 85 , reward =  12.2 , loss =  0.01078002\n",
      "episode = 90 , reward =  10.2 , loss =  0.021932228\n",
      "episode = 95 , reward =  10.8 , loss =  0.012967667\n",
      "episode = 100 , reward =  10.2 , loss =  0.0314303\n",
      "episode = 105 , reward =  108.6 , loss =  0.03422718\n",
      "episode = 110 , reward =  8.8 , loss =  0.062481306\n",
      "episode = 115 , reward =  10.2 , loss =  0.046023186\n",
      "episode = 120 , reward =  10.6 , loss =  0.03153433\n",
      "episode = 125 , reward =  10.0 , loss =  0.03821678\n",
      "episode = 130 , reward =  27.2 , loss =  0.3693934\n",
      "episode = 135 , reward =  10.0 , loss =  0.08912426\n",
      "episode = 140 , reward =  58.2 , loss =  0.09328194\n",
      "episode = 145 , reward =  29.0 , loss =  0.09491545\n",
      "episode = 150 , reward =  16.6 , loss =  0.10571242\n",
      "episode = 155 , reward =  11.2 , loss =  0.104691386\n",
      "episode = 160 , reward =  10.8 , loss =  0.03917432\n",
      "episode = 165 , reward =  45.2 , loss =  0.17078866\n",
      "episode = 170 , reward =  47.4 , loss =  0.14554304\n",
      "episode = 175 , reward =  11.0 , loss =  0.10376774\n",
      "episode = 180 , reward =  70.8 , loss =  0.046376273\n",
      "episode = 185 , reward =  21.6 , loss =  0.13720231\n",
      "episode = 190 , reward =  47.4 , loss =  0.11384136\n",
      "episode = 195 , reward =  67.0 , loss =  0.30040628\n",
      "episode = 200 , reward =  88.6 , loss =  0.29471877\n",
      "episode = 205 , reward =  94.4 , loss =  0.14433798\n",
      "episode = 210 , reward =  98.6 , loss =  0.28082678\n",
      "episode = 215 , reward =  83.4 , loss =  0.26446292\n",
      "episode = 220 , reward =  84.0 , loss =  0.21152453\n",
      "episode = 225 , reward =  84.2 , loss =  0.2075605\n",
      "episode = 230 , reward =  87.8 , loss =  0.47819054\n",
      "episode = 235 , reward =  86.2 , loss =  0.1267534\n",
      "episode = 240 , reward =  86.8 , loss =  0.19947872\n",
      "episode = 245 , reward =  90.4 , loss =  0.6814543\n",
      "episode = 250 , reward =  93.6 , loss =  0.19969757\n",
      "episode = 255 , reward =  97.4 , loss =  0.28560743\n",
      "episode = 260 , reward =  89.4 , loss =  0.4151714\n",
      "episode = 265 , reward =  83.8 , loss =  0.3544453\n",
      "episode = 270 , reward =  84.6 , loss =  0.5651881\n",
      "episode = 275 , reward =  93.6 , loss =  0.6201362\n",
      "episode = 280 , reward =  96.6 , loss =  0.33839226\n",
      "episode = 285 , reward =  85.4 , loss =  0.50514746\n",
      "episode = 290 , reward =  97.6 , loss =  0.7951169\n",
      "episode = 295 , reward =  97.2 , loss =  0.3709161\n",
      "episode = 300 , reward =  74.2 , loss =  0.43809107\n",
      "episode = 305 , reward =  74.6 , loss =  0.50803727\n",
      "episode = 310 , reward =  89.8 , loss =  0.62693155\n",
      "episode = 315 , reward =  87.6 , loss =  0.44681877\n",
      "episode = 320 , reward =  98.8 , loss =  0.76343393\n",
      "episode = 325 , reward =  95.2 , loss =  0.8449657\n",
      "episode = 330 , reward =  77.0 , loss =  0.6403842\n",
      "episode = 335 , reward =  77.2 , loss =  0.68560064\n",
      "episode = 340 , reward =  90.0 , loss =  0.78479016\n",
      "episode = 345 , reward =  63.8 , loss =  0.8928282\n",
      "episode = 350 , reward =  89.8 , loss =  0.59756565\n",
      "episode = 355 , reward =  70.0 , loss =  0.82942796\n",
      "episode = 360 , reward =  90.4 , loss =  0.26688778\n",
      "episode = 365 , reward =  90.0 , loss =  0.524071\n",
      "episode = 370 , reward =  89.8 , loss =  1.101928\n",
      "episode = 375 , reward =  75.8 , loss =  1.2195518\n",
      "episode = 380 , reward =  32.6 , loss =  0.8530391\n",
      "episode = 385 , reward =  70.8 , loss =  1.278343\n",
      "episode = 390 , reward =  17.0 , loss =  0.6399125\n",
      "episode = 395 , reward =  17.8 , loss =  1.1613045\n",
      "episode = 400 , reward =  23.8 , loss =  1.0404929\n",
      "episode = 405 , reward =  33.2 , loss =  1.3132056\n",
      "episode = 410 , reward =  46.6 , loss =  1.5427078\n",
      "episode = 415 , reward =  56.0 , loss =  1.2518042\n",
      "episode = 420 , reward =  18.4 , loss =  0.8139531\n",
      "episode = 425 , reward =  17.2 , loss =  0.57061327\n",
      "episode = 430 , reward =  16.2 , loss =  1.3962979\n",
      "episode = 435 , reward =  16.2 , loss =  0.8568332\n",
      "episode = 440 , reward =  18.4 , loss =  1.0993427\n",
      "episode = 445 , reward =  14.6 , loss =  1.9962071\n",
      "episode = 450 , reward =  74.4 , loss =  0.94677323\n",
      "episode = 455 , reward =  26.6 , loss =  1.145727\n",
      "episode = 460 , reward =  28.6 , loss =  1.6600982\n",
      "episode = 465 , reward =  24.0 , loss =  1.793835\n",
      "episode = 470 , reward =  72.6 , loss =  1.9479742\n",
      "episode = 475 , reward =  33.0 , loss =  0.6500893\n",
      "episode = 480 , reward =  46.6 , loss =  1.9980242\n",
      "episode = 485 , reward =  24.8 , loss =  0.36545476\n",
      "episode = 490 , reward =  15.8 , loss =  2.0608425\n",
      "episode = 495 , reward =  57.8 , loss =  2.9672287\n",
      "episode = 500 , reward =  47.8 , loss =  2.0285828\n",
      "episode = 505 , reward =  40.0 , loss =  1.3456502\n",
      "episode = 510 , reward =  79.6 , loss =  2.1529937\n",
      "episode = 515 , reward =  19.4 , loss =  1.1829209\n",
      "episode = 520 , reward =  37.4 , loss =  2.4148228\n",
      "episode = 525 , reward =  86.2 , loss =  2.0424378\n",
      "episode = 530 , reward =  94.6 , loss =  2.5535085\n",
      "episode = 535 , reward =  86.6 , loss =  1.6756152\n",
      "episode = 540 , reward =  83.4 , loss =  2.7373557\n",
      "episode = 545 , reward =  23.8 , loss =  2.707067\n",
      "episode = 550 , reward =  19.4 , loss =  2.6239746\n",
      "episode = 555 , reward =  33.8 , loss =  0.4273208\n",
      "episode = 560 , reward =  71.6 , loss =  2.766599\n",
      "episode = 565 , reward =  74.6 , loss =  4.1619787\n",
      "episode = 570 , reward =  19.2 , loss =  3.3656733\n",
      "episode = 575 , reward =  18.0 , loss =  1.8266845\n",
      "episode = 580 , reward =  36.6 , loss =  1.992899\n",
      "episode = 585 , reward =  33.2 , loss =  2.0670307\n",
      "episode = 590 , reward =  12.6 , loss =  1.3844116\n",
      "episode = 595 , reward =  32.4 , loss =  3.1631584\n",
      "episode = 600 , reward =  61.2 , loss =  2.2282581\n",
      "episode = 605 , reward =  18.6 , loss =  2.132196\n",
      "episode = 610 , reward =  15.8 , loss =  1.932518\n",
      "episode = 615 , reward =  17.4 , loss =  5.455474\n",
      "episode = 620 , reward =  22.8 , loss =  3.2334783\n",
      "episode = 625 , reward =  14.8 , loss =  1.1727521\n",
      "episode = 630 , reward =  21.2 , loss =  1.4538949\n",
      "episode = 635 , reward =  17.4 , loss =  2.486111\n",
      "episode = 640 , reward =  13.6 , loss =  3.2106996\n",
      "episode = 645 , reward =  46.0 , loss =  4.0992055\n",
      "episode = 650 , reward =  88.4 , loss =  4.6359453\n",
      "episode = 655 , reward =  21.4 , loss =  2.1626222\n",
      "episode = 660 , reward =  17.0 , loss =  2.6486962\n",
      "episode = 665 , reward =  87.2 , loss =  2.2924924\n",
      "episode = 670 , reward =  19.4 , loss =  4.861801\n",
      "episode = 675 , reward =  14.6 , loss =  2.4263988\n",
      "episode = 680 , reward =  15.6 , loss =  2.639293\n",
      "episode = 685 , reward =  11.2 , loss =  3.168953\n",
      "episode = 690 , reward =  42.6 , loss =  7.4559484\n",
      "episode = 695 , reward =  90.4 , loss =  3.8956149\n",
      "episode = 700 , reward =  13.8 , loss =  4.019764\n",
      "episode = 705 , reward =  28.8 , loss =  2.1712258\n",
      "episode = 710 , reward =  20.4 , loss =  5.6736264\n",
      "episode = 715 , reward =  90.2 , loss =  4.709684\n",
      "episode = 720 , reward =  16.2 , loss =  3.4057622\n",
      "episode = 725 , reward =  16.6 , loss =  3.895588\n",
      "episode = 730 , reward =  42.2 , loss =  4.7717924\n",
      "episode = 735 , reward =  91.2 , loss =  2.4323606\n",
      "episode = 740 , reward =  14.8 , loss =  4.639759\n",
      "episode = 745 , reward =  14.4 , loss =  4.76573\n",
      "episode = 750 , reward =  12.4 , loss =  3.2746239\n",
      "episode = 755 , reward =  43.2 , loss =  2.7963793\n",
      "episode = 760 , reward =  13.2 , loss =  5.7071233\n",
      "episode = 765 , reward =  14.2 , loss =  8.8452835\n",
      "episode = 770 , reward =  96.8 , loss =  6.7278557\n",
      "episode = 775 , reward =  94.2 , loss =  5.661653\n",
      "episode = 780 , reward =  94.6 , loss =  1.9047261\n",
      "episode = 785 , reward =  57.2 , loss =  5.1415043\n",
      "episode = 790 , reward =  90.4 , loss =  6.6351748\n",
      "episode = 795 , reward =  92.6 , loss =  6.1012\n",
      "episode = 800 , reward =  43.2 , loss =  8.712814\n",
      "episode = 805 , reward =  30.4 , loss =  4.3885665\n",
      "episode = 810 , reward =  34.6 , loss =  5.287024\n",
      "episode = 815 , reward =  89.0 , loss =  5.7211123\n",
      "episode = 820 , reward =  39.2 , loss =  4.355441\n",
      "episode = 825 , reward =  77.8 , loss =  4.7822924\n",
      "episode = 830 , reward =  28.0 , loss =  2.8011174\n",
      "episode = 835 , reward =  19.2 , loss =  5.598882\n",
      "episode = 840 , reward =  22.4 , loss =  4.159275\n",
      "episode = 845 , reward =  21.8 , loss =  10.496974\n",
      "episode = 850 , reward =  22.0 , loss =  5.2699504\n",
      "episode = 855 , reward =  23.2 , loss =  5.279834\n",
      "episode = 860 , reward =  21.2 , loss =  4.4497843\n",
      "episode = 865 , reward =  22.4 , loss =  8.718525\n",
      "episode = 870 , reward =  22.6 , loss =  7.0413823\n",
      "episode = 875 , reward =  23.4 , loss =  4.8312902\n",
      "episode = 880 , reward =  20.2 , loss =  6.756065\n",
      "episode = 885 , reward =  21.2 , loss =  9.006758\n",
      "episode = 890 , reward =  25.4 , loss =  5.9304686\n",
      "episode = 895 , reward =  21.4 , loss =  5.540938\n",
      "episode = 900 , reward =  58.2 , loss =  6.0195975\n",
      "episode = 905 , reward =  41.0 , loss =  5.964744\n",
      "episode = 910 , reward =  94.2 , loss =  5.909616\n"
     ]
    }
   ],
   "source": [
    "EVAL_EVERY = 5\n",
    "REWARD_THRESHOLD = 199\n",
    "\n",
    "def train():\n",
    "    state = env.reset()\n",
    "    epsilon = EPSILON_START\n",
    "    ep = 0\n",
    "    total_time = 0\n",
    "    losses = []\n",
    "    rewards_ = []\n",
    "    while ep < N_EPISODES:\n",
    "        action = choose_action(state, epsilon)\n",
    "\n",
    "        # take action and update replay buffer and networks\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        loss = update(state, action, reward, next_state, done)\n",
    "\n",
    "        # update state\n",
    "        state = next_state\n",
    "\n",
    "        # end episode if done\n",
    "        if done:\n",
    "            state = env.reset()\n",
    "            ep   += 1\n",
    "            if ( (ep+1)% EVAL_EVERY == 0):\n",
    "                rewards = eval_dqn()\n",
    "                print(\"episode =\", ep+1, \", reward = \", np.mean(rewards), \", loss = \", loss)\n",
    "                if np.mean(rewards) >= REWARD_THRESHOLD:\n",
    "                    break\n",
    "\n",
    "            # update target network\n",
    "            if ep % UPDATE_TARGET_EVERY == 0:\n",
    "                target_net.load_state_dict(q_net.state_dict())\n",
    "            # decrease epsilon\n",
    "            epsilon = EPSILON_MIN + (EPSILON_START - EPSILON_MIN) * \\\n",
    "                            np.exp(-1. * ep / DECREASE_EPSILON )    \n",
    "            losses += [loss]\n",
    "            rewards_ += [rewards]\n",
    "            \n",
    "        total_time += 1\n",
    "    return losses, rewards_ \n",
    "\n",
    "# Run the training loop\n",
    "losses, rewards_ = train()\n",
    "\n",
    "# Evaluate the final policy\n",
    "rewards = eval_dqn(20)\n",
    "print(\"\")\n",
    "print(\"mean reward after training = \", np.mean(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd27e48f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x11c879790>]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot([i for i in range(200)],losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90708d1",
   "metadata": {},
   "source": [
    "#### Question 5\n",
    "\n",
    "Experiment the policy network.\n",
    "\n",
    "(Showing a video with a jupyter notebook, you may try this cell with Chrome/Chromium instead of Firefox. Otherwise, you may skip this question.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1680e663",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-15 12:21:33.658 Python[93185:2355311] ApplePersistenceIgnoreState: Existing state will not be touched. New state will be written to /var/folders/pj/txbvyslx6zg5hgwg2f3zqm_00000gn/T/org.python.python.savedState\n"
     ]
    }
   ],
   "source": [
    "def show_video():\n",
    "    html = []\n",
    "    for mp4 in Path(\"videos\").glob(\"*.mp4\"):\n",
    "        video_b64 = base64.b64encode(mp4.read_bytes())\n",
    "        html.append('''<video alt=\"{}\" autoplay \n",
    "                      loop controls style=\"height: 400px;\">\n",
    "                      <source src=\"data:video/mp4;base64,{}\" type=\"video/mp4\" />\n",
    "                 </video>'''.format(mp4, video_b64.decode('ascii')))\n",
    "    ipythondisplay.display(ipythondisplay.HTML(data=\"<br>\".join(html)))\n",
    "    \n",
    "env = Monitor(env, './videos', force=True, video_callable=lambda episode: True)\n",
    "\n",
    "for episode in range(1):\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    while not done:\n",
    "        action = choose_action(state, 0.0)\n",
    "        state, reward, done, info = env.step(action)\n",
    "env.close()\n",
    "#show_video()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbb94bc",
   "metadata": {},
   "source": [
    "### Experiments: Do It Yourself"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eae9fdf",
   "metadata": {},
   "source": [
    "Remember the set of global parameters:\n",
    "```\n",
    "# Environment\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "# Discount factor\n",
    "GAMMA = 0.99\n",
    "\n",
    "# Batch size\n",
    "BATCH_SIZE = 256\n",
    "# Capacity of the replay buffer\n",
    "BUFFER_CAPACITY = 16384 # 10000\n",
    "# Update target net every ... episodes\n",
    "UPDATE_TARGET_EVERY = 32 # 20\n",
    "\n",
    "# Initial value of epsilon\n",
    "EPSILON_START = 1.0\n",
    "# Parameter to decrease epsilon\n",
    "DECREASE_EPSILON = 200\n",
    "# Minimum value of epislon\n",
    "EPSILON_MIN = 0.05\n",
    "\n",
    "# Number of training episodes\n",
    "N_EPISODES = 200\n",
    "\n",
    "# Learning rate\n",
    "LEARNING_RATE = 0.1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ba2f8e",
   "metadata": {},
   "source": [
    "#### Question 6\n",
    "\n",
    "Craft an experiment and study the influence of the `BUFFER_CAPACITY` on the learning process (speed of *convergence*, training curves...) \n",
    "\n",
    "**Answer 6**  \n",
    "Un buffer trop petit crée beaucoup d'instabilité car seul les dernières parties sont jouées. Une mauvaise partie efface donc toute la mémoire et reset l'apprentissage\n",
    "Un buffer trop grand prend de la place en mémoire et en temps (sélectionner un batch) tout en étant mauvais car il garde des parties trop vieilles et donc inutiles.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b1533f",
   "metadata": {},
   "source": [
    "#### Question 7\n",
    "\n",
    "Craft an experiment and study the influence of the `UPDATE_TARGET_EVERY` on the learning process (speed of *convergence*, training curves...)\n",
    "\n",
    "\n",
    "***Answer 7**  \n",
    "Plus UPDATE_TARGET_EVERY moins il faut jouer de parties avant d'actualiser le réseau de neurones.  \n",
    "Si UPDATE_TARGET_EVERY est trop petit on ne joue pas assez entre chaque actualisation, on actualise donc le réseau sur de vieilles parties ce qui le rend instable car il peut désapprendre.  \n",
    "Si UPDATE_TARGET_EVERY est trop grand on pert inutilement du temps.  \n",
    "Comme CartPole est très simple, un UPDATE_TARGET_EVERY de 1 convient ici pour un buffer pas trop grand.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f197f12",
   "metadata": {},
   "source": [
    "#### Question 8\n",
    "\n",
    "If you have the computer power to do so, try to do a grid search on those two hyper-parameters and comment the results. Otherwise, study the influence of another hyper-parameter.\n",
    "\n",
    "**Answer 8**  \n",
    "N'ayant pas la puissance de calcul pour réaliser un grid-search j'ai décidé de regarder l'influence de BATCH_SIZE Un BATCH_SIZE influence directement sur le temps d'apprentissage.  \n",
    "Si BATCH_SIZE est trop petit on risque de jouer trop par rapport à l'apprentissage. BATCH_SIZE doit donc dépendre indirectement de UPDATE_TARGET_EVERY.  \n",
    "Si BATCH_SIZE est trop grand le temps d'apprentissage et inutilement long. On s'expose aussi à des problèmes de surapprentissage car l'intégralité d'une partie peut se retrouver dans le batch. Le réseau risque alors d'apprendre \"par cœur\" cette partie au lieu de l'extrapoler ce qui lui est néfaste.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc76ffb",
   "metadata": {},
   "source": [
    "## Discretization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b35d83",
   "metadata": {},
   "source": [
    "It is natural to use a function approximator like a neural network to approximate the $Q$ function in a continuous environment. Another natural but unscalable way to do handle continuous state-action space is **discretization**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de1525f",
   "metadata": {},
   "source": [
    "Discretize the environment of your choice (cartpole or mountain car or both) and run one of the algorithms that you know to compute an approximation of the optimal $Q$ function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c32ad0",
   "metadata": {},
   "source": [
    "Once you are satisfied with your results, you may plot the *optimal phase diagram* of the system. For instance, you may get something like this for the mountain car environment.\n",
    "\n",
    "![Phase diagram](./img/phase_plot.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "eff4d0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Everything! This is an introduction to research and development"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
